{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "in iteration 0the accuracy is : 0.1015\n",
      "in iteration 1the accuracy is : 0.1158\n",
      "in iteration 2the accuracy is : 0.1278\n",
      "in iteration 3the accuracy is : 0.1402\n",
      "in iteration 4the accuracy is : 0.1511\n",
      "in iteration 5the accuracy is : 0.1591\n",
      "in iteration 6the accuracy is : 0.167\n",
      "in iteration 7the accuracy is : 0.175\n",
      "in iteration 8the accuracy is : 0.1833\n",
      "in iteration 9the accuracy is : 0.1906\n",
      "in iteration 10the accuracy is : 0.198\n",
      "in iteration 11the accuracy is : 0.2048\n",
      "in iteration 12the accuracy is : 0.2097\n",
      "in iteration 13the accuracy is : 0.2148\n",
      "in iteration 14the accuracy is : 0.2216\n",
      "in iteration 15the accuracy is : 0.2278\n",
      "in iteration 16the accuracy is : 0.2334\n",
      "in iteration 17the accuracy is : 0.2392\n",
      "in iteration 18the accuracy is : 0.2456\n",
      "in iteration 19the accuracy is : 0.2518\n",
      "in iteration 20the accuracy is : 0.2578\n",
      "in iteration 21the accuracy is : 0.2656\n",
      "in iteration 22the accuracy is : 0.2731\n",
      "in iteration 23the accuracy is : 0.2834\n",
      "in iteration 24the accuracy is : 0.2923\n",
      "in iteration 25the accuracy is : 0.3013\n",
      "in iteration 26the accuracy is : 0.3128\n",
      "in iteration 27the accuracy is : 0.3231\n",
      "in iteration 28the accuracy is : 0.3318\n",
      "in iteration 29the accuracy is : 0.3392\n",
      "in iteration 30the accuracy is : 0.3447\n",
      "in iteration 31the accuracy is : 0.3513\n",
      "in iteration 32the accuracy is : 0.3591\n",
      "in iteration 33the accuracy is : 0.3663\n",
      "in iteration 34the accuracy is : 0.3716\n",
      "in iteration 35the accuracy is : 0.3768\n",
      "in iteration 36the accuracy is : 0.3817\n",
      "in iteration 37the accuracy is : 0.3852\n",
      "in iteration 38the accuracy is : 0.3886\n",
      "in iteration 39the accuracy is : 0.3922\n",
      "in iteration 40the accuracy is : 0.3956\n",
      "in iteration 41the accuracy is : 0.3987\n",
      "in iteration 42the accuracy is : 0.4029\n",
      "in iteration 43the accuracy is : 0.4082\n",
      "in iteration 44the accuracy is : 0.4117\n",
      "in iteration 45the accuracy is : 0.4174\n",
      "in iteration 46the accuracy is : 0.4233\n",
      "in iteration 47the accuracy is : 0.4328\n",
      "in iteration 48the accuracy is : 0.4386\n",
      "in iteration 49the accuracy is : 0.4472\n"
     ]
    }
   ],
   "source": [
    "# 数据集介绍\n",
    "# MNIST数据集，100k的训练数据，10k的预测数据，数据由tensorflow中的examples.tutorials.mnist读取 \n",
    "# 数据集介绍：：Yann LeCun's website\n",
    "# 由28*28的像素组成输入特征，输出特征为0-9的数字\n",
    "\n",
    "# 可调节参数：\n",
    "# --------\n",
    "# batch_size, initial_weight,二次损失函数,learning_rate,epoch_n\n",
    "# --------\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)\n",
    "\n",
    "# mini_batch的大小\n",
    "batch_size = 100\n",
    "batch_n = mnist.train.num_examples // batch_size\n",
    "\n",
    "# 创建一个命名空间\n",
    "with tf.name_scope(\"input\"):\n",
    "    # # 定义两个placeholder用来feed数据，分别代表x和y --784列和10列(one-hot)\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name = \"x_input\")\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name = \"y_input\")\n",
    "\n",
    "# tensorflow中定义一个函数，summary变量的所有特点    \n",
    "def variables_summary(variable):\n",
    "    with tf.name_scope(\"summary\"):\n",
    "        # 通过summary.scalar建立对标量的追踪\n",
    "        mean = tf.reduce_mean(variable)\n",
    "        tf.summary.scalar(\"mean\", mean)\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(variable - mean)))\n",
    "        tf.summary.scalar(\"stddev\", stddev)\n",
    "        tf.summary.histogram(\"histogram\", variable)\n",
    "\n",
    "# # ----\n",
    "# # 构建多分类回归\n",
    "# # 定义weight和bias，初始化分别为正态随机和0.0\n",
    "with tf.name_scope(\"lager\"):\n",
    "    with tf.name_scope(\"weight\"):\n",
    "        initial_weight = tf.random_normal([784, 10])\n",
    "        weight = tf.Variable(initial_weight)\n",
    "        variables_summary(weight) # 利用summary函数观察一下weight的变化情况\n",
    "    with tf.name_scope(\"bias\"):\n",
    "        bias = tf.Variable(tf.zeros([10]))\n",
    "        variables_summary(bias) # 利用summary观察一下bias的变化情况\n",
    "    with tf.name_scope(\"mat_and_plus\"):\n",
    "        a = tf.matmul(x, weight) + bias\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        y_head = tf.nn.softmax(a)\n",
    "\n",
    "# # 定义二次损失函数并依据梯度下降法进行训练 -- 这样梯度下降的train就变成了x和y的函数\n",
    "with tf.name_scope(\"loss\"):\n",
    "    learning_rate = 0.1\n",
    "    loss = tf.reduce_mean(tf.square(y - y_head))\n",
    "    tf.summary.scalar(\"loss\", loss) # 利用summary观察一下loss的变化\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_head, 1)) # tf.argmax找到x中等于1的最大的id\n",
    "    correction = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # tf.cast 转换类型，将bool转为float，从而求得准确率\n",
    "    tf.summary.scalar(\"accuracy\", correction)\n",
    "\n",
    "# 将summary的scalar merge\n",
    "merge = tf.summary.merge_all()\n",
    "    \n",
    "    \n",
    "# 迭代500次，进行mini_batch梯度下降\n",
    "epoch_n = 50\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    writer = tf.summary.FileWriter(logdir = \"d://tensorboardLogDir/logs/\", graph = session.graph) # 最好不要放到带中文字符的路径\n",
    "    for step in range(epoch_n):\n",
    "        for batch in range(batch_n):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, summary = session.run([train, merge], feed_dict= {x: batch_x, y: batch_y})\n",
    "        writer.add_summary(summary, step) # 每执行一个周期将summary中的信息写入log、\n",
    "        corr = session.run(correction, feed_dict= {x: mnist.test.images, y: mnist.test.labels}) # 基于测试集对准确率进行测试\n",
    "        print(\"in iteration \" + str(step) + \"the accuracy is : \" + str(corr)) # 打印准确率\n",
    "# 这里看似有问题，其实没问题，因为图没变，DAG对输入的batch依次执行梯度下降法，\n",
    "# 并执行epoch_n个周期，权重会更新epoch_n * batch_n次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
